# Comparing `tmp/torchrec_nightly-2023.7.19-py39-none-any.whl.zip` & `tmp/torchrec_nightly-2023.7.21-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,150 +1,150 @@
-Zip file size: 369966 bytes, number of entries: 148
--rw-r--r--  2.0 unx      811 b- defN 23-Jul-19 11:17 torchrec/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 23-Jul-19 11:17 torchrec/streamable.py
--rw-r--r--  2.0 unx      854 b- defN 23-Jul-19 11:17 torchrec/types.py
--rw-r--r--  2.0 unx     1153 b- defN 23-Jul-19 11:17 torchrec/datasets/__init__.py
--rw-r--r--  2.0 unx    41469 b- defN 23-Jul-19 11:17 torchrec/datasets/criteo.py
--rw-r--r--  2.0 unx     4548 b- defN 23-Jul-19 11:17 torchrec/datasets/movielens.py
--rw-r--r--  2.0 unx     6539 b- defN 23-Jul-19 11:17 torchrec/datasets/random.py
--rw-r--r--  2.0 unx    10909 b- defN 23-Jul-19 11:17 torchrec/datasets/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-19 11:17 torchrec/datasets/scripts/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-Jul-19 11:17 torchrec/datasets/scripts/contiguous_preproc_criteo.py
--rw-r--r--  2.0 unx     2847 b- defN 23-Jul-19 11:17 torchrec/datasets/scripts/npy_preproc_criteo.py
--rw-r--r--  2.0 unx     3077 b- defN 23-Jul-19 11:17 torchrec/datasets/scripts/shuffle_preproc_criteo.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-19 11:17 torchrec/datasets/test_utils/__init__.py
--rw-r--r--  2.0 unx     5308 b- defN 23-Jul-19 11:17 torchrec/datasets/test_utils/criteo_test_utils.py
--rw-r--r--  2.0 unx     1912 b- defN 23-Jul-19 11:17 torchrec/distributed/__init__.py
--rw-r--r--  2.0 unx    37307 b- defN 23-Jul-19 11:17 torchrec/distributed/batched_embedding_kernel.py
--rw-r--r--  2.0 unx     2069 b- defN 23-Jul-19 11:17 torchrec/distributed/collective_utils.py
--rw-r--r--  2.0 unx     4988 b- defN 23-Jul-19 11:17 torchrec/distributed/comm.py
--rw-r--r--  2.0 unx    55918 b- defN 23-Jul-19 11:17 torchrec/distributed/comm_ops.py
--rw-r--r--  2.0 unx    36967 b- defN 23-Jul-19 11:17 torchrec/distributed/dist_data.py
--rw-r--r--  2.0 unx    32295 b- defN 23-Jul-19 11:17 torchrec/distributed/embedding.py
--rw-r--r--  2.0 unx     4947 b- defN 23-Jul-19 11:17 torchrec/distributed/embedding_kernel.py
--rw-r--r--  2.0 unx    29185 b- defN 23-Jul-19 11:17 torchrec/distributed/embedding_lookup.py
--rw-r--r--  2.0 unx    19035 b- defN 23-Jul-19 11:17 torchrec/distributed/embedding_sharding.py
--rw-r--r--  2.0 unx    36853 b- defN 23-Jul-19 11:17 torchrec/distributed/embedding_tower_sharding.py
--rw-r--r--  2.0 unx    15030 b- defN 23-Jul-19 11:17 torchrec/distributed/embedding_types.py
--rw-r--r--  2.0 unx    37369 b- defN 23-Jul-19 11:17 torchrec/distributed/embeddingbag.py
--rw-r--r--  2.0 unx     7373 b- defN 23-Jul-19 11:17 torchrec/distributed/fbgemm_qcomm_codec.py
--rw-r--r--  2.0 unx     6137 b- defN 23-Jul-19 11:17 torchrec/distributed/fp_embeddingbag.py
--rw-r--r--  2.0 unx     5243 b- defN 23-Jul-19 11:17 torchrec/distributed/fused_embedding.py
--rw-r--r--  2.0 unx     5080 b- defN 23-Jul-19 11:17 torchrec/distributed/fused_embeddingbag.py
--rw-r--r--  2.0 unx     2271 b- defN 23-Jul-19 11:17 torchrec/distributed/fused_params.py
--rw-r--r--  2.0 unx     3807 b- defN 23-Jul-19 11:17 torchrec/distributed/grouped_position_weighted.py
--rw-r--r--  2.0 unx    10997 b- defN 23-Jul-19 11:17 torchrec/distributed/mc_embeddingbag.py
--rw-r--r--  2.0 unx    19750 b- defN 23-Jul-19 11:17 torchrec/distributed/model_parallel.py
--rw-r--r--  2.0 unx    20748 b- defN 23-Jul-19 11:17 torchrec/distributed/quant_embedding.py
--rw-r--r--  2.0 unx    15112 b- defN 23-Jul-19 11:17 torchrec/distributed/quant_embedding_kernel.py
--rw-r--r--  2.0 unx    12621 b- defN 23-Jul-19 11:17 torchrec/distributed/quant_embeddingbag.py
--rw-r--r--  2.0 unx    13928 b- defN 23-Jul-19 11:17 torchrec/distributed/quant_state.py
--rw-r--r--  2.0 unx     9261 b- defN 23-Jul-19 11:17 torchrec/distributed/shard.py
--rw-r--r--  2.0 unx    19646 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding_plan.py
--rw-r--r--  2.0 unx    32061 b- defN 23-Jul-19 11:17 torchrec/distributed/train_pipeline.py
--rw-r--r--  2.0 unx    27865 b- defN 23-Jul-19 11:17 torchrec/distributed/types.py
--rw-r--r--  2.0 unx    15470 b- defN 23-Jul-19 11:17 torchrec/distributed/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-19 11:17 torchrec/distributed/composable/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 23-Jul-19 11:17 torchrec/distributed/composable/table_batched_embedding_slice.py
--rw-r--r--  2.0 unx     1025 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/__init__.py
--rw-r--r--  2.0 unx     3135 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/constants.py
--rw-r--r--  2.0 unx    11430 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/enumerators.py
--rw-r--r--  2.0 unx    12642 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/partitioners.py
--rw-r--r--  2.0 unx      835 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/perf_models.py
--rw-r--r--  2.0 unx    13530 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/planners.py
--rw-r--r--  2.0 unx    11236 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/proposers.py
--rw-r--r--  2.0 unx    41291 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/shard_estimators.py
--rw-r--r--  2.0 unx    23791 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/stats.py
--rw-r--r--  2.0 unx     9643 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/storage_reservations.py
--rw-r--r--  2.0 unx    14582 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/types.py
--rw-r--r--  2.0 unx     1119 b- defN 23-Jul-19 11:17 torchrec/distributed/planner/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/__init__.py
--rw-r--r--  2.0 unx     2479 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/cw_sequence_sharding.py
--rw-r--r--  2.0 unx    12945 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/cw_sharding.py
--rw-r--r--  2.0 unx     2802 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/dp_sequence_sharding.py
--rw-r--r--  2.0 unx     7681 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/dp_sharding.py
--rw-r--r--  2.0 unx     7640 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/rw_sequence_sharding.py
--rw-r--r--  2.0 unx    17911 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/rw_sharding.py
--rw-r--r--  2.0 unx     3627 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/sequence_sharding.py
--rw-r--r--  2.0 unx     7632 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/tw_sequence_sharding.py
--rw-r--r--  2.0 unx    16097 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/tw_sharding.py
--rw-r--r--  2.0 unx     1284 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/twcw_sharding.py
--rw-r--r--  2.0 unx    19871 b- defN 23-Jul-19 11:17 torchrec/distributed/sharding/twrw_sharding.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-19 11:17 torchrec/distributed/test_utils/__init__.py
--rw-r--r--  2.0 unx    11237 b- defN 23-Jul-19 11:17 torchrec/distributed/test_utils/infer_utils.py
--rw-r--r--  2.0 unx     4868 b- defN 23-Jul-19 11:17 torchrec/distributed/test_utils/multi_process.py
--rw-r--r--  2.0 unx    34091 b- defN 23-Jul-19 11:17 torchrec/distributed/test_utils/test_model.py
--rw-r--r--  2.0 unx    11197 b- defN 23-Jul-19 11:17 torchrec/distributed/test_utils/test_model_parallel.py
--rw-r--r--  2.0 unx    25310 b- defN 23-Jul-19 11:17 torchrec/distributed/test_utils/test_model_parallel_base.py
--rw-r--r--  2.0 unx    15367 b- defN 23-Jul-19 11:17 torchrec/distributed/test_utils/test_sharding.py
--rw-r--r--  2.0 unx      422 b- defN 23-Jul-19 11:17 torchrec/fx/__init__.py
--rw-r--r--  2.0 unx     6451 b- defN 23-Jul-19 11:17 torchrec/fx/tracer.py
--rw-r--r--  2.0 unx     4524 b- defN 23-Jul-19 11:17 torchrec/fx/utils.py
--rw-r--r--  2.0 unx     1223 b- defN 23-Jul-19 11:17 torchrec/inference/__init__.py
--rw-r--r--  2.0 unx     3614 b- defN 23-Jul-19 11:17 torchrec/inference/client.py
--rw-r--r--  2.0 unx     3957 b- defN 23-Jul-19 11:17 torchrec/inference/model_packager.py
--rw-r--r--  2.0 unx     8068 b- defN 23-Jul-19 11:17 torchrec/inference/modules.py
--rw-r--r--  2.0 unx     3797 b- defN 23-Jul-19 11:17 torchrec/inference/state_dict_transform.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-19 11:17 torchrec/metrics/__init__.py
--rw-r--r--  2.0 unx     4168 b- defN 23-Jul-19 11:17 torchrec/metrics/accuracy.py
--rw-r--r--  2.0 unx    12549 b- defN 23-Jul-19 11:17 torchrec/metrics/auc.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Jul-19 11:17 torchrec/metrics/calibration.py
--rw-r--r--  2.0 unx     3465 b- defN 23-Jul-19 11:17 torchrec/metrics/ctr.py
--rw-r--r--  2.0 unx     3836 b- defN 23-Jul-19 11:17 torchrec/metrics/mae.py
--rw-r--r--  2.0 unx    17990 b- defN 23-Jul-19 11:17 torchrec/metrics/metric_module.py
--rw-r--r--  2.0 unx     6796 b- defN 23-Jul-19 11:17 torchrec/metrics/metrics_config.py
--rw-r--r--  2.0 unx     3731 b- defN 23-Jul-19 11:17 torchrec/metrics/metrics_namespace.py
--rw-r--r--  2.0 unx     3904 b- defN 23-Jul-19 11:17 torchrec/metrics/model_utils.py
--rw-r--r--  2.0 unx     4631 b- defN 23-Jul-19 11:17 torchrec/metrics/mse.py
--rw-r--r--  2.0 unx     5605 b- defN 23-Jul-19 11:17 torchrec/metrics/multiclass_recall.py
--rw-r--r--  2.0 unx     8735 b- defN 23-Jul-19 11:17 torchrec/metrics/ndcg.py
--rw-r--r--  2.0 unx     6811 b- defN 23-Jul-19 11:17 torchrec/metrics/ne.py
--rw-r--r--  2.0 unx    33554 b- defN 23-Jul-19 11:17 torchrec/metrics/rec_metric.py
--rw-r--r--  2.0 unx    10490 b- defN 23-Jul-19 11:17 torchrec/metrics/recall_session.py
--rw-r--r--  2.0 unx     6057 b- defN 23-Jul-19 11:17 torchrec/metrics/throughput.py
--rw-r--r--  2.0 unx    11160 b- defN 23-Jul-19 11:17 torchrec/metrics/tower_qps.py
--rw-r--r--  2.0 unx     2867 b- defN 23-Jul-19 11:17 torchrec/metrics/weighted_avg.py
--rw-r--r--  2.0 unx    16441 b- defN 23-Jul-19 11:17 torchrec/metrics/test_utils/__init__.py
--rw-r--r--  2.0 unx      913 b- defN 23-Jul-19 11:17 torchrec/models/__init__.py
--rw-r--r--  2.0 unx    11410 b- defN 23-Jul-19 11:17 torchrec/models/deepfm.py
--rw-r--r--  2.0 unx    29965 b- defN 23-Jul-19 11:17 torchrec/models/dlrm.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jul-19 11:17 torchrec/models/experimental/__init__.py
--rw-r--r--  2.0 unx     9825 b- defN 23-Jul-19 11:17 torchrec/models/experimental/test_transformerdlrm.py
--rw-r--r--  2.0 unx     7434 b- defN 23-Jul-19 11:17 torchrec/models/experimental/transformerdlrm.py
--rw-r--r--  2.0 unx     1179 b- defN 23-Jul-19 11:17 torchrec/modules/__init__.py
--rw-r--r--  2.0 unx     1456 b- defN 23-Jul-19 11:17 torchrec/modules/activation.py
--rw-r--r--  2.0 unx    14636 b- defN 23-Jul-19 11:17 torchrec/modules/crossnet.py
--rw-r--r--  2.0 unx     8415 b- defN 23-Jul-19 11:17 torchrec/modules/deepfm.py
--rw-r--r--  2.0 unx     6468 b- defN 23-Jul-19 11:17 torchrec/modules/embedding_configs.py
--rw-r--r--  2.0 unx    14021 b- defN 23-Jul-19 11:17 torchrec/modules/embedding_modules.py
--rw-r--r--  2.0 unx     4858 b- defN 23-Jul-19 11:17 torchrec/modules/embedding_tower.py
--rw-r--r--  2.0 unx    12360 b- defN 23-Jul-19 11:17 torchrec/modules/feature_processor.py
--rw-r--r--  2.0 unx     4832 b- defN 23-Jul-19 11:17 torchrec/modules/feature_processor_.py
--rw-r--r--  2.0 unx     4403 b- defN 23-Jul-19 11:17 torchrec/modules/fp_embedding_modules.py
--rw-r--r--  2.0 unx    31545 b- defN 23-Jul-19 11:17 torchrec/modules/fused_embedding_modules.py
--rw-r--r--  2.0 unx    10541 b- defN 23-Jul-19 11:17 torchrec/modules/lazy_extension.py
--rw-r--r--  2.0 unx     3143 b- defN 23-Jul-19 11:17 torchrec/modules/managed_collision_modules.py
--rw-r--r--  2.0 unx     5124 b- defN 23-Jul-19 11:17 torchrec/modules/mc_embedding_modules.py
--rw-r--r--  2.0 unx     6309 b- defN 23-Jul-19 11:17 torchrec/modules/mlp.py
--rw-r--r--  2.0 unx     3897 b- defN 23-Jul-19 11:17 torchrec/modules/utils.py
--rw-r--r--  2.0 unx     1639 b- defN 23-Jul-19 11:17 torchrec/optim/__init__.py
--rw-r--r--  2.0 unx     2012 b- defN 23-Jul-19 11:17 torchrec/optim/apply_optimizer_in_backward.py
--rw-r--r--  2.0 unx     1569 b- defN 23-Jul-19 11:17 torchrec/optim/clipping.py
--rw-r--r--  2.0 unx     1353 b- defN 23-Jul-19 11:17 torchrec/optim/fused.py
--rw-r--r--  2.0 unx    16069 b- defN 23-Jul-19 11:17 torchrec/optim/keyed.py
--rw-r--r--  2.0 unx     4420 b- defN 23-Jul-19 11:17 torchrec/optim/optimizers.py
--rw-r--r--  2.0 unx     7405 b- defN 23-Jul-19 11:17 torchrec/optim/rowwise_adagrad.py
--rw-r--r--  2.0 unx     4865 b- defN 23-Jul-19 11:17 torchrec/optim/warmup.py
--rw-r--r--  2.0 unx      560 b- defN 23-Jul-19 11:17 torchrec/optim/test_utils/__init__.py
--rw-r--r--  2.0 unx     1140 b- defN 23-Jul-19 11:17 torchrec/quant/__init__.py
--rw-r--r--  2.0 unx    26618 b- defN 23-Jul-19 11:17 torchrec/quant/embedding_modules.py
--rw-r--r--  2.0 unx     4292 b- defN 23-Jul-19 11:17 torchrec/quant/utils.py
--rw-r--r--  2.0 unx     1163 b- defN 23-Jul-19 11:17 torchrec/sparse/__init__.py
--rw-r--r--  2.0 unx    56412 b- defN 23-Jul-19 11:17 torchrec/sparse/jagged_tensor.py
--rw-r--r--  2.0 unx     1430 b- defN 23-Jul-19 11:17 torchrec/sparse/test_utils/__init__.py
--rw-r--r--  2.0 unx     5661 b- defN 23-Jul-19 11:17 torchrec/test_utils/__init__.py
--rw-r--r--  2.0 unx     1530 b- defN 23-Jul-19 11:22 torchrec_nightly-2023.7.19.dist-info/LICENSE
--rw-r--r--  2.0 unx     5012 b- defN 23-Jul-19 11:22 torchrec_nightly-2023.7.19.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-Jul-19 11:22 torchrec_nightly-2023.7.19.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-Jul-19 11:22 torchrec_nightly-2023.7.19.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    13823 b- defN 23-Jul-19 11:22 torchrec_nightly-2023.7.19.dist-info/RECORD
-148 files, 1520709 bytes uncompressed, 347856 bytes compressed:  77.1%
+Zip file size: 371372 bytes, number of entries: 148
+-rw-r--r--  2.0 unx      811 b- defN 23-Jul-21 11:17 torchrec/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-Jul-21 11:17 torchrec/streamable.py
+-rw-r--r--  2.0 unx      854 b- defN 23-Jul-21 11:17 torchrec/types.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-Jul-21 11:17 torchrec/datasets/__init__.py
+-rw-r--r--  2.0 unx    41469 b- defN 23-Jul-21 11:17 torchrec/datasets/criteo.py
+-rw-r--r--  2.0 unx     4548 b- defN 23-Jul-21 11:17 torchrec/datasets/movielens.py
+-rw-r--r--  2.0 unx     6539 b- defN 23-Jul-21 11:17 torchrec/datasets/random.py
+-rw-r--r--  2.0 unx    10909 b- defN 23-Jul-21 11:17 torchrec/datasets/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-21 11:17 torchrec/datasets/scripts/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-Jul-21 11:17 torchrec/datasets/scripts/contiguous_preproc_criteo.py
+-rw-r--r--  2.0 unx     2847 b- defN 23-Jul-21 11:17 torchrec/datasets/scripts/npy_preproc_criteo.py
+-rw-r--r--  2.0 unx     3077 b- defN 23-Jul-21 11:17 torchrec/datasets/scripts/shuffle_preproc_criteo.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-21 11:17 torchrec/datasets/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5308 b- defN 23-Jul-21 11:17 torchrec/datasets/test_utils/criteo_test_utils.py
+-rw-r--r--  2.0 unx     1912 b- defN 23-Jul-21 11:17 torchrec/distributed/__init__.py
+-rw-r--r--  2.0 unx    37307 b- defN 23-Jul-21 11:17 torchrec/distributed/batched_embedding_kernel.py
+-rw-r--r--  2.0 unx     2069 b- defN 23-Jul-21 11:17 torchrec/distributed/collective_utils.py
+-rw-r--r--  2.0 unx     4988 b- defN 23-Jul-21 11:17 torchrec/distributed/comm.py
+-rw-r--r--  2.0 unx    56136 b- defN 23-Jul-21 11:17 torchrec/distributed/comm_ops.py
+-rw-r--r--  2.0 unx    37051 b- defN 23-Jul-21 11:17 torchrec/distributed/dist_data.py
+-rw-r--r--  2.0 unx    32295 b- defN 23-Jul-21 11:17 torchrec/distributed/embedding.py
+-rw-r--r--  2.0 unx     4947 b- defN 23-Jul-21 11:17 torchrec/distributed/embedding_kernel.py
+-rw-r--r--  2.0 unx    29185 b- defN 23-Jul-21 11:17 torchrec/distributed/embedding_lookup.py
+-rw-r--r--  2.0 unx    20950 b- defN 23-Jul-21 11:17 torchrec/distributed/embedding_sharding.py
+-rw-r--r--  2.0 unx    36853 b- defN 23-Jul-21 11:17 torchrec/distributed/embedding_tower_sharding.py
+-rw-r--r--  2.0 unx    15030 b- defN 23-Jul-21 11:17 torchrec/distributed/embedding_types.py
+-rw-r--r--  2.0 unx    37369 b- defN 23-Jul-21 11:17 torchrec/distributed/embeddingbag.py
+-rw-r--r--  2.0 unx     7373 b- defN 23-Jul-21 11:17 torchrec/distributed/fbgemm_qcomm_codec.py
+-rw-r--r--  2.0 unx     6137 b- defN 23-Jul-21 11:17 torchrec/distributed/fp_embeddingbag.py
+-rw-r--r--  2.0 unx     5243 b- defN 23-Jul-21 11:17 torchrec/distributed/fused_embedding.py
+-rw-r--r--  2.0 unx     5080 b- defN 23-Jul-21 11:17 torchrec/distributed/fused_embeddingbag.py
+-rw-r--r--  2.0 unx     2271 b- defN 23-Jul-21 11:17 torchrec/distributed/fused_params.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-Jul-21 11:17 torchrec/distributed/grouped_position_weighted.py
+-rw-r--r--  2.0 unx    10997 b- defN 23-Jul-21 11:17 torchrec/distributed/mc_embeddingbag.py
+-rw-r--r--  2.0 unx    19750 b- defN 23-Jul-21 11:17 torchrec/distributed/model_parallel.py
+-rw-r--r--  2.0 unx    20748 b- defN 23-Jul-21 11:17 torchrec/distributed/quant_embedding.py
+-rw-r--r--  2.0 unx    15112 b- defN 23-Jul-21 11:17 torchrec/distributed/quant_embedding_kernel.py
+-rw-r--r--  2.0 unx    12621 b- defN 23-Jul-21 11:17 torchrec/distributed/quant_embeddingbag.py
+-rw-r--r--  2.0 unx    13928 b- defN 23-Jul-21 11:17 torchrec/distributed/quant_state.py
+-rw-r--r--  2.0 unx     9261 b- defN 23-Jul-21 11:17 torchrec/distributed/shard.py
+-rw-r--r--  2.0 unx    19646 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding_plan.py
+-rw-r--r--  2.0 unx    39755 b- defN 23-Jul-21 11:17 torchrec/distributed/train_pipeline.py
+-rw-r--r--  2.0 unx    27865 b- defN 23-Jul-21 11:17 torchrec/distributed/types.py
+-rw-r--r--  2.0 unx    15470 b- defN 23-Jul-21 11:17 torchrec/distributed/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-21 11:17 torchrec/distributed/composable/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 23-Jul-21 11:17 torchrec/distributed/composable/table_batched_embedding_slice.py
+-rw-r--r--  2.0 unx     1025 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/__init__.py
+-rw-r--r--  2.0 unx     3135 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/constants.py
+-rw-r--r--  2.0 unx    11430 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/enumerators.py
+-rw-r--r--  2.0 unx    12642 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/partitioners.py
+-rw-r--r--  2.0 unx      835 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/perf_models.py
+-rw-r--r--  2.0 unx    13530 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/planners.py
+-rw-r--r--  2.0 unx    11236 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/proposers.py
+-rw-r--r--  2.0 unx    41291 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/shard_estimators.py
+-rw-r--r--  2.0 unx    23791 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/stats.py
+-rw-r--r--  2.0 unx     9643 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/storage_reservations.py
+-rw-r--r--  2.0 unx    14582 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/types.py
+-rw-r--r--  2.0 unx     1119 b- defN 23-Jul-21 11:17 torchrec/distributed/planner/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/__init__.py
+-rw-r--r--  2.0 unx     2479 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/cw_sequence_sharding.py
+-rw-r--r--  2.0 unx    12945 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/cw_sharding.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/dp_sequence_sharding.py
+-rw-r--r--  2.0 unx     7681 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/dp_sharding.py
+-rw-r--r--  2.0 unx     7640 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/rw_sequence_sharding.py
+-rw-r--r--  2.0 unx    17911 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/rw_sharding.py
+-rw-r--r--  2.0 unx     3627 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/sequence_sharding.py
+-rw-r--r--  2.0 unx     7632 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/tw_sequence_sharding.py
+-rw-r--r--  2.0 unx    16097 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/tw_sharding.py
+-rw-r--r--  2.0 unx     1284 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/twcw_sharding.py
+-rw-r--r--  2.0 unx    19871 b- defN 23-Jul-21 11:17 torchrec/distributed/sharding/twrw_sharding.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-21 11:17 torchrec/distributed/test_utils/__init__.py
+-rw-r--r--  2.0 unx    11237 b- defN 23-Jul-21 11:17 torchrec/distributed/test_utils/infer_utils.py
+-rw-r--r--  2.0 unx     4868 b- defN 23-Jul-21 11:17 torchrec/distributed/test_utils/multi_process.py
+-rw-r--r--  2.0 unx    34091 b- defN 23-Jul-21 11:17 torchrec/distributed/test_utils/test_model.py
+-rw-r--r--  2.0 unx    11197 b- defN 23-Jul-21 11:17 torchrec/distributed/test_utils/test_model_parallel.py
+-rw-r--r--  2.0 unx    25310 b- defN 23-Jul-21 11:17 torchrec/distributed/test_utils/test_model_parallel_base.py
+-rw-r--r--  2.0 unx    15367 b- defN 23-Jul-21 11:17 torchrec/distributed/test_utils/test_sharding.py
+-rw-r--r--  2.0 unx      422 b- defN 23-Jul-21 11:17 torchrec/fx/__init__.py
+-rw-r--r--  2.0 unx     6451 b- defN 23-Jul-21 11:17 torchrec/fx/tracer.py
+-rw-r--r--  2.0 unx     4524 b- defN 23-Jul-21 11:17 torchrec/fx/utils.py
+-rw-r--r--  2.0 unx     1223 b- defN 23-Jul-21 11:17 torchrec/inference/__init__.py
+-rw-r--r--  2.0 unx     3614 b- defN 23-Jul-21 11:17 torchrec/inference/client.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-Jul-21 11:17 torchrec/inference/model_packager.py
+-rw-r--r--  2.0 unx     8068 b- defN 23-Jul-21 11:17 torchrec/inference/modules.py
+-rw-r--r--  2.0 unx     3797 b- defN 23-Jul-21 11:17 torchrec/inference/state_dict_transform.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-21 11:17 torchrec/metrics/__init__.py
+-rw-r--r--  2.0 unx     4168 b- defN 23-Jul-21 11:17 torchrec/metrics/accuracy.py
+-rw-r--r--  2.0 unx    12549 b- defN 23-Jul-21 11:17 torchrec/metrics/auc.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Jul-21 11:17 torchrec/metrics/calibration.py
+-rw-r--r--  2.0 unx     3465 b- defN 23-Jul-21 11:17 torchrec/metrics/ctr.py
+-rw-r--r--  2.0 unx     3836 b- defN 23-Jul-21 11:17 torchrec/metrics/mae.py
+-rw-r--r--  2.0 unx    17990 b- defN 23-Jul-21 11:17 torchrec/metrics/metric_module.py
+-rw-r--r--  2.0 unx     6796 b- defN 23-Jul-21 11:17 torchrec/metrics/metrics_config.py
+-rw-r--r--  2.0 unx     3731 b- defN 23-Jul-21 11:17 torchrec/metrics/metrics_namespace.py
+-rw-r--r--  2.0 unx     3904 b- defN 23-Jul-21 11:17 torchrec/metrics/model_utils.py
+-rw-r--r--  2.0 unx     4631 b- defN 23-Jul-21 11:17 torchrec/metrics/mse.py
+-rw-r--r--  2.0 unx     5605 b- defN 23-Jul-21 11:17 torchrec/metrics/multiclass_recall.py
+-rw-r--r--  2.0 unx     8735 b- defN 23-Jul-21 11:17 torchrec/metrics/ndcg.py
+-rw-r--r--  2.0 unx     6811 b- defN 23-Jul-21 11:17 torchrec/metrics/ne.py
+-rw-r--r--  2.0 unx    33554 b- defN 23-Jul-21 11:17 torchrec/metrics/rec_metric.py
+-rw-r--r--  2.0 unx    10490 b- defN 23-Jul-21 11:17 torchrec/metrics/recall_session.py
+-rw-r--r--  2.0 unx     6057 b- defN 23-Jul-21 11:17 torchrec/metrics/throughput.py
+-rw-r--r--  2.0 unx    11160 b- defN 23-Jul-21 11:17 torchrec/metrics/tower_qps.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-Jul-21 11:17 torchrec/metrics/weighted_avg.py
+-rw-r--r--  2.0 unx    16441 b- defN 23-Jul-21 11:17 torchrec/metrics/test_utils/__init__.py
+-rw-r--r--  2.0 unx      913 b- defN 23-Jul-21 11:17 torchrec/models/__init__.py
+-rw-r--r--  2.0 unx    11410 b- defN 23-Jul-21 11:17 torchrec/models/deepfm.py
+-rw-r--r--  2.0 unx    29965 b- defN 23-Jul-21 11:17 torchrec/models/dlrm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-21 11:17 torchrec/models/experimental/__init__.py
+-rw-r--r--  2.0 unx     9825 b- defN 23-Jul-21 11:17 torchrec/models/experimental/test_transformerdlrm.py
+-rw-r--r--  2.0 unx     7434 b- defN 23-Jul-21 11:17 torchrec/models/experimental/transformerdlrm.py
+-rw-r--r--  2.0 unx     1179 b- defN 23-Jul-21 11:17 torchrec/modules/__init__.py
+-rw-r--r--  2.0 unx     1456 b- defN 23-Jul-21 11:17 torchrec/modules/activation.py
+-rw-r--r--  2.0 unx    14636 b- defN 23-Jul-21 11:17 torchrec/modules/crossnet.py
+-rw-r--r--  2.0 unx     8415 b- defN 23-Jul-21 11:17 torchrec/modules/deepfm.py
+-rw-r--r--  2.0 unx     6468 b- defN 23-Jul-21 11:17 torchrec/modules/embedding_configs.py
+-rw-r--r--  2.0 unx    14021 b- defN 23-Jul-21 11:17 torchrec/modules/embedding_modules.py
+-rw-r--r--  2.0 unx     4858 b- defN 23-Jul-21 11:17 torchrec/modules/embedding_tower.py
+-rw-r--r--  2.0 unx    12360 b- defN 23-Jul-21 11:17 torchrec/modules/feature_processor.py
+-rw-r--r--  2.0 unx     4832 b- defN 23-Jul-21 11:17 torchrec/modules/feature_processor_.py
+-rw-r--r--  2.0 unx     4403 b- defN 23-Jul-21 11:17 torchrec/modules/fp_embedding_modules.py
+-rw-r--r--  2.0 unx    31545 b- defN 23-Jul-21 11:17 torchrec/modules/fused_embedding_modules.py
+-rw-r--r--  2.0 unx    10541 b- defN 23-Jul-21 11:17 torchrec/modules/lazy_extension.py
+-rw-r--r--  2.0 unx     3143 b- defN 23-Jul-21 11:17 torchrec/modules/managed_collision_modules.py
+-rw-r--r--  2.0 unx     5124 b- defN 23-Jul-21 11:17 torchrec/modules/mc_embedding_modules.py
+-rw-r--r--  2.0 unx     6309 b- defN 23-Jul-21 11:17 torchrec/modules/mlp.py
+-rw-r--r--  2.0 unx     3897 b- defN 23-Jul-21 11:17 torchrec/modules/utils.py
+-rw-r--r--  2.0 unx     1639 b- defN 23-Jul-21 11:17 torchrec/optim/__init__.py
+-rw-r--r--  2.0 unx     2012 b- defN 23-Jul-21 11:17 torchrec/optim/apply_optimizer_in_backward.py
+-rw-r--r--  2.0 unx     1569 b- defN 23-Jul-21 11:17 torchrec/optim/clipping.py
+-rw-r--r--  2.0 unx     1353 b- defN 23-Jul-21 11:17 torchrec/optim/fused.py
+-rw-r--r--  2.0 unx    16069 b- defN 23-Jul-21 11:17 torchrec/optim/keyed.py
+-rw-r--r--  2.0 unx     4420 b- defN 23-Jul-21 11:17 torchrec/optim/optimizers.py
+-rw-r--r--  2.0 unx     7405 b- defN 23-Jul-21 11:17 torchrec/optim/rowwise_adagrad.py
+-rw-r--r--  2.0 unx     4865 b- defN 23-Jul-21 11:17 torchrec/optim/warmup.py
+-rw-r--r--  2.0 unx      560 b- defN 23-Jul-21 11:17 torchrec/optim/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1140 b- defN 23-Jul-21 11:17 torchrec/quant/__init__.py
+-rw-r--r--  2.0 unx    26618 b- defN 23-Jul-21 11:17 torchrec/quant/embedding_modules.py
+-rw-r--r--  2.0 unx     4292 b- defN 23-Jul-21 11:17 torchrec/quant/utils.py
+-rw-r--r--  2.0 unx     1163 b- defN 23-Jul-21 11:17 torchrec/sparse/__init__.py
+-rw-r--r--  2.0 unx    56412 b- defN 23-Jul-21 11:17 torchrec/sparse/jagged_tensor.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-Jul-21 11:17 torchrec/sparse/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-Jul-21 11:17 torchrec/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-Jul-21 11:20 torchrec_nightly-2023.7.21.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5012 b- defN 23-Jul-21 11:20 torchrec_nightly-2023.7.21.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-Jul-21 11:20 torchrec_nightly-2023.7.21.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-21 11:20 torchrec_nightly-2023.7.21.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    13823 b- defN 23-Jul-21 11:20 torchrec_nightly-2023.7.21.dist-info/RECORD
+148 files, 1530620 bytes uncompressed, 349262 bytes compressed:  77.2%
```

## zipnote {}

```diff
@@ -423,23 +423,23 @@
 
 Filename: torchrec/sparse/test_utils/__init__.py
 Comment: 
 
 Filename: torchrec/test_utils/__init__.py
 Comment: 
 
-Filename: torchrec_nightly-2023.7.19.dist-info/LICENSE
+Filename: torchrec_nightly-2023.7.21.dist-info/LICENSE
 Comment: 
 
-Filename: torchrec_nightly-2023.7.19.dist-info/METADATA
+Filename: torchrec_nightly-2023.7.21.dist-info/METADATA
 Comment: 
 
-Filename: torchrec_nightly-2023.7.19.dist-info/WHEEL
+Filename: torchrec_nightly-2023.7.21.dist-info/WHEEL
 Comment: 
 
-Filename: torchrec_nightly-2023.7.19.dist-info/top_level.txt
+Filename: torchrec_nightly-2023.7.21.dist-info/top_level.txt
 Comment: 
 
-Filename: torchrec_nightly-2023.7.19.dist-info/RECORD
+Filename: torchrec_nightly-2023.7.21.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchrec/distributed/comm_ops.py

```diff
@@ -195,30 +195,30 @@
     input_sizes: List[torch.Size]
     codecs: Optional[QuantizedCommCodecs] = None
 
 
 @dataclass
 class ReduceScatterBaseInfo(object):
     """
-    The data class that collects the attributes when calling the `reduce_scatter_base_pooled`
-    operation.
+    The data class that collects the attributes when calling the
+    `reduce_scatter_base_pooled` operation.
 
     Attributes:
         input_sizes (torch.Size): the sizes of the input flatten tensor.
     """
 
     input_sizes: torch.Size
     codecs: Optional[QuantizedCommCodecs] = None
 
 
 @dataclass
 class AllGatherBaseInfo(object):
     """
-    The data class that collects the attributes when calling the `all_gatther_base_pooled`
-    operation.
+    The data class that collects the attributes when calling the
+    `all_gatther_base_pooled` operation.
 
     Attributes:
         input_size (int): the size of the input tensor.
     """
 
     input_size: torch.Size
     codecs: Optional[QuantizedCommCodecs] = None
@@ -227,32 +227,32 @@
 @dataclass
 class ReduceScatterVInfo(object):
     """
     The data class that collects the attributes when calling the `reduce_scatter_v_pooled`
     operation.
 
     Attributes:
-        input_sizes (List[torch.Size]): the sizes of the input tensors. This remembers the
+        input_sizes (List[torch.Size]): the sizes of the input tensors. This saves the
             sizes of the input tensors when running the backward pass and producing the
             gradient.
-        input_splits (List[int]): the splits of the input tensors along dim0.
-        total_input_size: (List[int]): total input size
+        input_splits (List[int]): the splits of the input tensors along dim 0.
+        total_input_size: (List[int]): total input size.
     """
 
     input_sizes: List[torch.Size]
     input_splits: List[int]
     equal_splits: bool
     total_input_size: List[int]
     codecs: Optional[QuantizedCommCodecs]
 
 
 @dataclass
 class All2AllDenseInfo(object):
     """
-    The data class that collects the attributes when calling the alltoall_dense
+    The data class that collects the attributes when calling the `alltoall_dense`
     operation.
     """
 
     output_splits: List[int]
     batch_size: int
     input_shape: List[int]
     input_splits: List[int]
@@ -284,29 +284,28 @@
     Performs AlltoAll operation for a single pooled embedding tensor. Each process
     splits the input pooled embeddings tensor based on the world size, and then scatters
     the split list to all processes in the group. Then concatenates the received tensors
     from all processes in the group and returns a single output tensor.
 
     Args:
         a2a_pooled_embs_tensor (Tensor): input pooled embeddings. Must be pooled
-            together before passing into this function. Its shape is B x D_local_sum,
-            where D_local_sum is the dimension sum of all the local
-            embedding tables.
+            together before passing into this function. Its shape is `B x D_local_sum`,
+            where `D_local_sum` is the dimension sum of all the local embedding tables.
         batch_size_per_rank (List[int]): batch size in each rank.
         dim_sum_per_rank (List[int]): number of features (sum of dimensions) of the
             embedding in each rank.
         dim_sum_per_rank_tensor (Optional[Tensor]): the tensor version of
             `dim_sum_per_rank`, this is only used by the fast kernel of
             `_recat_pooled_embedding_grad_out`.
         cumsum_dim_sum_per_rank_tensor (Optional[Tensor]): cumulative sum of
             `dim_sum_per_rank`, this is only used by the fast kernel of
             `_recat_pooled_embedding_grad_out`.
-        group (Optional[dist.ProcessGroup]): The process group to work on. If None, the
+        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the
             default process group will be used.
-        codecs: Optional[QuantizedCommCodecs]: Quantized communication codecs
+        codecs: Optional[QuantizedCommCodecs]: Quantized communication codecs.
 
     Returns:
         Awaitable[List[Tensor]]: async work handle (`Awaitable`), which can be `wait()` later to get the resulting tensor.
 
     .. warning::
         `alltoall_pooled` is experimental and subject to change.
     """
@@ -353,20 +352,20 @@
 
     Args:
         a2a_sequence_embs_tensor (Tensor): input embeddings.
         forward_recat_tensor (Tensor): recat tensor for forward.
         backward_recat_tensor (Tensor): recat tensor for backward.
         lengths_after_sparse_data_all2all (Tensor): lengths of sparse features after
             AlltoAll.
-        input_splits (Tensor): input splits.
-        output_splits (Tensor): output splits.
-        variable_batch_size (bool): whether variable batch size is enabled
-        group (Optional[dist.ProcessGroup]): The process group to work on. If None, the
+        input_splits (List[int]): input splits.
+        output_splits (List[int]): output splits.
+        variable_batch_size (bool): whether variable batch size is enabled.
+        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the
             default process group will be used.
-        codecs: Optional[QuantizedCommCodecs]: Quantized communication codecs
+        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.
 
     Returns:
         Awaitable[List[Tensor]]: async work handle (`Awaitable`), which can be `wait()` later to get the resulting tensor.
 
     .. warning::
         `alltoall_sequence` is experimental and subject to change.
     """
@@ -409,16 +408,17 @@
         inputs (List[Tensor]): list of tensors to scatter, one per rank. The tensors in
             the list usually have different lengths.
         out_split (Optional[List[int]]): output split sizes (or dim_sum_per_rank), if
             not specified, we will use `per_rank_split_lengths` to construct a output
             split with the assumption that all the embs have the same dimension.
         per_rank_split_lengths (Optional[List[int]]): split lengths per rank. If not
             specified, the `out_split` must be specified.
-        group (Optional[dist.ProcessGroup]): The process group to work on. If None, the
+        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the
             default process group will be used.
+        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.
 
     Returns:
         Awaitable[List[Tensor]]: async work handle (`Awaitable`), which can be `wait()` later to get the resulting list of tensors.
 
     .. warning::
         `alltoallv` is experimental and subject to change.
     """
@@ -464,16 +464,17 @@
     """
     Performs reduce-scatter operation for a pooled embeddings tensor split into world
     size number of chunks. The result of the reduce operation gets scattered to all
     processes in the group.
 
     Args:
         inputs (List[Tensor]): list of tensors to scatter, one per rank.
-        group (Optional[dist.ProcessGroup]): The process group to work on. If None, the
+        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the
             default process group will be used.
+        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.
 
     Returns:
         Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.
 
     .. warning::
         `reduce_scatter_pooled` is experimental and subject to change.
     """
@@ -489,58 +490,61 @@
         input_sizes=[tensor.size() for tensor in inputs], codecs=codecs
     )
     ReduceScatter_Req.apply(group, myreq, rsi, *inputs)
     return myreq
 
 
 def reduce_scatter_base_pooled(
-    inputs: Tensor,
+    input: Tensor,
     group: Optional[dist.ProcessGroup] = None,
     codecs: Optional[QuantizedCommCodecs] = None,
 ) -> Awaitable[Tensor]:
     """
-    Reduces then scatters a flattened pooled embeddings tensor to all processes in a group.
-    Input tensor is of size output tensor size times world size.
+    Reduces then scatters a flattened pooled embeddings tensor to all processes in a
+    group.
+    Input tensor is of size `output_tensor_size * world_size`.
 
     Args:
-        inputs (Tensor): flattened tensor to scatter, .
-        group (Optional[dist.ProcessGroup]): The process group to work on. If None, the
+        input (Tensor): flattened tensor to scatter.
+        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the
             default process group will be used.
+        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.
 
     Returns:
         Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.
 
     .. warning::
         `reduce_scatter_base_pooled` is experimental and subject to change.
     """
 
     if group is None:
         group = dist.distributed_c10d._get_default_group()
 
     if dist.get_world_size(group) <= 1:
-        return NoWait(inputs)
+        return NoWait(input)
 
-    myreq = Request(group, device=inputs.device)
-    rsi = ReduceScatterBaseInfo(input_sizes=inputs.size(), codecs=codecs)
-    ReduceScatterBase_Req.apply(group, myreq, rsi, inputs)
+    myreq = Request(group, device=input.device)
+    rsi = ReduceScatterBaseInfo(input_sizes=input.size(), codecs=codecs)
+    ReduceScatterBase_Req.apply(group, myreq, rsi, input)
     return myreq
 
 
 def all_gather_base_pooled(
     input: Tensor,
     group: Optional[dist.ProcessGroup] = None,
     codecs: Optional[QuantizedCommCodecs] = None,
 ) -> Awaitable[Tensor]:
     """
-    All-gathers tensors from all processes in a group to form a flattened pooled embeddings tensor.
-    Input tensor is of size output tensor size divided by world size.
+    All-gathers tensors from all processes in a group to form a flattened pooled
+    embeddings tensor.
+    Input tensor is of size `output_tensor_size / world_size`.
 
     Args:
-        input (Tensor): tensor to gather, .
-        group (Optional[dist.ProcessGroup]): The process group to work on. If None, the
+        input (Tensor): tensor to gather.
+        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the
             default process group will be used.
 
     Returns:
         Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.
 
     .. warning::
         `all_gather_base_pooled` is experimental and subject to change.
@@ -561,22 +565,22 @@
 def reduce_scatter_v_pooled(
     input: Tensor,
     input_splits: List[int],
     group: Optional[dist.ProcessGroup] = None,
     codecs: Optional[QuantizedCommCodecs] = None,
 ) -> Awaitable[Tensor]:
     """
-    Performs reduce-scatter-v operation for a pooled embeddings tensor split unevenly into world
-    size number of chunks. The result of the reduce operation gets scattered to all
-    processes in the group according to input_splits.
+    Performs reduce-scatter-v operation for a pooled embeddings tensor split unevenly
+    into world size number of chunks. The result of the reduce operation gets scattered
+    to all processes in the group according to `input_splits`.
 
     Args:
-        input (Tensor): tensors to scatter, one per rank.
+        input (Tensor): tensor to scatter.
         input_splits (List[int]): input splits.
-        group (Optional[dist.ProcessGroup]): The process group to work on. If None, the
+        group (Optional[dist.ProcessGroup]): the process group to work on. If None, the
             default process group will be used.
 
     Returns:
         Awaitable[Tensor]: async work handle (Awaitable), which can be `wait()` later to get the resulting tensor.
 
     .. warning::
         `reduce_scatter_v_pooled` is experimental and subject to change.
```

## torchrec/distributed/dist_data.py

```diff
@@ -785,26 +785,26 @@
 
 class PooledEmbeddingsReduceScatter(nn.Module):
     """
     The module class that wraps reduce-scatter communication primitives for pooled
     embedding communication in row-wise and twrw sharding.
 
     For pooled embeddings, we have a local model-parallel output tensor with a layout of
-    [num_buckets x batch_size, dimension]. We need to sum over num_buckets dimension
-    across batches. We split tensor along the first dimension into unequal chunks (tensor
-    slices of different buckets) according to input_splits and reduce them into the output
-    tensor and scatter the results for corresponding ranks.
+    `[num_buckets x batch_size, dimension]`. We need to sum over `num_buckets` dimension
+    across batches. We split the tensor along the first dimension into unequal chunks
+    (tensor slices of different buckets) according to `input_splits` and reduce them
+    into the output tensor and scatter the results for corresponding ranks.
 
     The class returns the async `Awaitable` handle for pooled embeddings tensor.
-    The reduce-scatter-v is only available for NCCL backend.
+    The `reduce-scatter-v` operation is only available for NCCL backend.
 
     Args:
-        pg (dist.ProcessGroup): The process group that the reduce-scatter communication
+        pg (dist.ProcessGroup): the process group that the reduce-scatter communication
             happens within.
-        codecs (Optional[QuantizedCommCodecs]): Quantization codec
+        codecs (Optional[QuantizedCommCodecs]): quantized communication codecs.
 
      Example::
 
         init_distributed(rank=rank, size=2, backend="nccl")
         pg = dist.new_group(backend="nccl")
         input = torch.randn(2 * 2, 2)
         input_splits = [1,3]
@@ -825,16 +825,17 @@
     def forward(
         self, local_embs: torch.Tensor, input_splits: Optional[List[int]] = None
     ) -> PooledEmbeddingsAwaitable:
         """
         Performs reduce scatter operation on pooled embeddings tensor.
 
         Args:
-            local_embs (torch.Tensor): tensor of shape [num_buckets x batch_size, dimension].
-            input_splits (Optional[List[int]]): list of splits for local_embs dim0.
+            local_embs (torch.Tensor): tensor of shape
+                `[num_buckets * batch_size, dimension]`.
+            input_splits (Optional[List[int]]): list of splits for `local_embs` dim 0.
 
         Returns:
             PooledEmbeddingsAwaitable: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
         """
 
         if input_splits and len(set(input_splits)) > 1:
             tensor_awaitable = reduce_scatter_v_pooled(
@@ -845,25 +846,25 @@
                 local_embs, self._pg, codecs=self._codecs
             )
         return PooledEmbeddingsAwaitable(tensor_awaitable=tensor_awaitable)
 
 
 class PooledEmbeddingsAllGather(nn.Module):
     """
-    The module class that wraps all-gather communication primitive for pooled
-    embedding communication
+    The module class that wraps the all-gather communication primitive for pooled
+    embedding communication.
 
-    We have a local input tensor with a layout of
-    [batch_size, dimension]. We need to gather input tensors from all ranks into a flatten output tensor.
+    Provided a local input tensor with a layout of `[batch_size, dimension]`, we want to
+    gather input tensors from all ranks into a flattened output tensor.
 
     The class returns the async `Awaitable` handle for pooled embeddings tensor.
     The all-gather is only available for NCCL backend.
 
     Args:
-        pg (dist.ProcessGroup): The process group that the all-gather communication
+        pg (dist.ProcessGroup): the process group that the all-gather communication
             happens within.
 
     Example::
 
         init_distributed(rank=rank, size=2, backend="nccl")
         pg = dist.new_group(backend="nccl")
         input = torch.randn(2, 2)
@@ -882,15 +883,16 @@
         self._codecs = codecs
 
     def forward(self, local_emb: torch.Tensor) -> PooledEmbeddingsAwaitable:
         """
         Performs reduce scatter operation on pooled embeddings tensor.
 
         Args:
-            local_emb (torch.Tensor): tensor of shape [num_buckets x batch_size, dimension].
+            local_emb (torch.Tensor): tensor of shape
+                `[num_buckets x batch_size, dimension]`.
 
         Returns:
             PooledEmbeddingsAwaitable: awaitable of pooled embeddings of tensor of shape [batch_size, dimension].
         """
 
         tensor_awaitable = all_gather_base_pooled(
             local_emb, self._pg, codecs=self._codecs
```

## torchrec/distributed/embedding_sharding.py

```diff
@@ -368,32 +368,60 @@
             splits_list = self._splits_awaitable.wait()
             splits_per_awaitable = _split(splits_list, self._lengths)
         else:
             splits_per_awaitable = [[] for _ in range(len(self._lengths))]
         tensors_awaitables = []
         for splits, awaitable in zip(splits_per_awaitable, self._awaitables):
             if not splits:  # NoWait
-                assert isinstance(awaitable, Awaitable)
+                # pyre-fixme[16]: Item `KJTSplitsAllToAllMeta` of
+                #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                #  KJTSplitsAllToAllMeta]` has no attribute `wait`.
                 tensors_awaitables.append(awaitable.wait())
                 continue
             output_splits = splits[:-1]
             batch_size_per_rank = splits[-1]
-            assert isinstance(awaitable, KJTSplitsAllToAllMeta)
             tensors_awaitables.append(
                 KJTAllToAllTensorsAwaitable(
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `pg`.
                     pg=awaitable.pg,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `input`.
                     input=awaitable.input,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `splits`.
                     splits=awaitable.splits,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `input_splits`.
                     input_splits=awaitable.input_splits,
                     output_splits=output_splits,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `input_tensors`.
                     input_tensors=awaitable.input_tensors,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `labels`.
                     labels=awaitable.labels,
                     batch_size_per_rank=batch_size_per_rank,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `keys`.
                     keys=awaitable.keys,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `device`.
                     device=awaitable.device,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `stagger`.
                     stagger=awaitable.stagger,
                 )
             )
         output = []
         awaitables_per_output = _split(tensors_awaitables, self._output_lengths)
         for awaitables, ctx in zip(awaitables_per_output, self._contexts):
             _set_sharding_context(awaitables, ctx)
```

## torchrec/distributed/train_pipeline.py

```diff
@@ -23,25 +23,25 @@
     Generic,
     Iterator,
     List,
     Optional,
     Set,
     Tuple,
     TypeVar,
+    Union,
 )
 
 import torch
 from torch import distributed as dist
 from torch.autograd.profiler import record_function
 from torch.fx.node import Node
-from torchrec.distributed.dist_data import KJTAllToAll
+from torchrec.distributed.dist_data import KJTAllToAll, KJTAllToAllTensorsAwaitable
 from torchrec.distributed.embedding_sharding import (
-    FusedKJTListSplitsAwaitable,
+    KJTListAwaitable,
     KJTListSplitsAwaitable,
-    KJTSplitsAllToAllMeta,
 )
 from torchrec.distributed.model_parallel import DistributedModelParallel, ShardedModule
 from torchrec.distributed.types import Awaitable
 from torchrec.modules.feature_processor import BaseGroupedFeatureProcessor
 from torchrec.sparse.jagged_tensor import KeyedJaggedTensor
 from torchrec.streamable import Multistreamable, Pipelineable
 
@@ -173,14 +173,195 @@
 
     def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:
         if isinstance(m, ShardedModule) or module_qualified_name in self._leaf_modules:
             return True
         return super().is_leaf_module(m, module_qualified_name)
 
 
+# TODO: remove after packaging issue is resolved.
+class SplitsAllToAllAwaitable(Awaitable[List[List[int]]]):
+    def __init__(
+        self,
+        input_tensors: List[torch.Tensor],
+        pg: dist.ProcessGroup,
+    ) -> None:
+        super().__init__()
+        self.num_workers: int = pg.size()
+        with record_function("## all2all_data:kjt splits ##"):
+            self._output_tensor: torch.Tensor = torch.empty(
+                [self.num_workers * len(input_tensors)],
+                device=input_tensors[0].device,
+                dtype=input_tensors[0].dtype,
+            )
+            input_tensor = torch.stack(input_tensors, dim=1).flatten()
+            self._splits_awaitable: dist.Work = dist.all_to_all_single(
+                output=self._output_tensor,
+                input=input_tensor,
+                group=pg,
+                async_op=True,
+            )
+
+    def _wait_impl(self) -> List[List[int]]:
+        self._splits_awaitable.wait()
+        return self._output_tensor.view(self.num_workers, -1).T.tolist()
+
+
+# TODO: remove after packaging issue is resolved.
+C = TypeVar("C", bound=Multistreamable)
+T = TypeVar("T")
+
+
+# TODO: remove after packaging issue is resolved.
+def _set_sharding_context(
+    tensors_awaitables: List[Awaitable[KeyedJaggedTensor]],
+    ctx: C,
+) -> None:
+    for awaitable, sharding_context in zip(
+        tensors_awaitables,
+        getattr(ctx, "sharding_contexts", []),
+    ):
+        if isinstance(awaitable, KJTAllToAllTensorsAwaitable):
+            if hasattr(sharding_context, "batch_size_per_rank"):
+                sharding_context.batch_size_per_rank = awaitable._batch_size_per_rank
+            if hasattr(sharding_context, "input_splits"):
+                sharding_context.input_splits = awaitable._input_splits["values"]
+            if hasattr(sharding_context, "output_splits"):
+                sharding_context.output_splits = awaitable._output_splits["values"]
+            if hasattr(sharding_context, "sparse_features_recat"):
+                sharding_context.sparse_features_recat = awaitable._recat
+
+
+# TODO: remove after packaging issue is resolved.
+@dataclass
+class KJTSplitsAllToAllMeta:
+    pg: dist.ProcessGroup
+    input: KeyedJaggedTensor
+    splits: List[int]
+    splits_tensors: List[torch.Tensor]
+    input_splits: List[List[int]]
+    input_tensors: List[torch.Tensor]
+    labels: List[str]
+    keys: List[str]
+    device: torch.device
+    stagger: int
+
+
+# TODO: remove after packaging issue is resolved.
+def _split(flat_list: List[T], splits: List[int]) -> List[List[T]]:
+    return [
+        flat_list[sum(splits[:i]) : sum(splits[:i]) + n] for i, n in enumerate(splits)
+    ]
+
+
+# TODO: remove after packaging issue is resolved.
+class FusedKJTListSplitsAwaitable(Awaitable[List[KJTListAwaitable]]):
+    def __init__(
+        self,
+        requests: List[KJTListSplitsAwaitable[C]],
+        contexts: List[C],
+        pg: Optional[dist.ProcessGroup],
+    ) -> None:
+        super().__init__()
+        self._contexts = contexts
+        self._awaitables: List[
+            Union[KJTSplitsAllToAllMeta, Awaitable[Awaitable[KeyedJaggedTensor]]]
+        ] = [awaitable for request in requests for awaitable in request.awaitables]
+        self._output_lengths: List[int] = [
+            len(request.awaitables) for request in requests
+        ]
+        self._lengths: List[int] = [
+            len(awaitable.splits_tensors)
+            if isinstance(awaitable, KJTSplitsAllToAllMeta)
+            else 0
+            for awaitable in self._awaitables
+        ]
+        splits_tensors = [
+            splits_tensor
+            for awaitable in self._awaitables
+            for splits_tensor in (
+                awaitable.splits_tensors
+                if isinstance(awaitable, KJTSplitsAllToAllMeta)
+                else []
+            )
+        ]
+        self._splits_awaitable: Optional[SplitsAllToAllAwaitable] = (
+            SplitsAllToAllAwaitable(
+                input_tensors=splits_tensors,
+                pg=pg,
+            )
+            if splits_tensors and pg
+            else None
+        )
+
+    def _wait_impl(self) -> List[KJTListAwaitable]:
+        if self._splits_awaitable:
+            splits_list = self._splits_awaitable.wait()
+            splits_per_awaitable = _split(splits_list, self._lengths)
+        else:
+            splits_per_awaitable = [[] for _ in range(len(self._lengths))]
+        tensors_awaitables = []
+        for splits, awaitable in zip(splits_per_awaitable, self._awaitables):
+            if not splits:  # NoWait
+                # pyre-fixme[16]: Item `KJTSplitsAllToAllMeta` of
+                #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                #  KJTSplitsAllToAllMeta]` has no attribute `wait`.
+                tensors_awaitables.append(awaitable.wait())
+                continue
+            output_splits = splits[:-1]
+            batch_size_per_rank = splits[-1]
+            tensors_awaitables.append(
+                KJTAllToAllTensorsAwaitable(
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `pg`.
+                    pg=awaitable.pg,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `input`.
+                    input=awaitable.input,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `splits`.
+                    splits=awaitable.splits,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `input_splits`.
+                    input_splits=awaitable.input_splits,
+                    output_splits=output_splits,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `input_tensors`.
+                    input_tensors=awaitable.input_tensors,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `labels`.
+                    labels=awaitable.labels,
+                    batch_size_per_rank=batch_size_per_rank,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `keys`.
+                    keys=awaitable.keys,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `device`.
+                    device=awaitable.device,
+                    # pyre-fixme[16]: Item `Awaitable` of
+                    #  `Union[Awaitable[Awaitable[KeyedJaggedTensor]],
+                    #  KJTSplitsAllToAllMeta]` has no attribute `stagger`.
+                    stagger=awaitable.stagger,
+                )
+            )
+        output = []
+        awaitables_per_output = _split(tensors_awaitables, self._output_lengths)
+        for awaitables, ctx in zip(awaitables_per_output, self._contexts):
+            _set_sharding_context(awaitables, ctx)
+            output.append(KJTListAwaitable(awaitables))
+        return output
+
+
 @dataclass
 class TrainPipelineContext:
     """
     Context information for a `TrainPipelineSparseDist` instance.
 
     Attributes:
         input_dist_splits_requests (Dict[str, Awaitable[Any]]): Stores input dist
```

## Comparing `torchrec_nightly-2023.7.19.dist-info/LICENSE` & `torchrec_nightly-2023.7.21.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchrec_nightly-2023.7.19.dist-info/METADATA` & `torchrec_nightly-2023.7.21.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchrec-nightly
-Version: 2023.7.19
+Version: 2023.7.21
 Summary: Pytorch domain library for recommendation systems
 Home-page: https://github.com/pytorch/torchrec
 Author: TorchRec Team
 Author-email: packages@pytorch.org
 License: BSD-3
 Keywords: pytorch,recommendation systems,sharding
 Classifier: Development Status :: 4 - Beta
```

## Comparing `torchrec_nightly-2023.7.19.dist-info/RECORD` & `torchrec_nightly-2023.7.21.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -12,20 +12,20 @@
 torchrec/datasets/scripts/shuffle_preproc_criteo.py,sha256=PC1t5EkJkG6qu3ioewAVZM-Bnzo01HKMUH92IprFth0,3077
 torchrec/datasets/test_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/datasets/test_utils/criteo_test_utils.py,sha256=Ob2fJniGOsfbWNF_Gy2RJhrGAVnLAFPSlUTJOp2kay4,5308
 torchrec/distributed/__init__.py,sha256=VCy8GKOM-1dejxUWNSA3gozG3HQ4x5-Y9c9-WFbAMGg,1912
 torchrec/distributed/batched_embedding_kernel.py,sha256=uBFpJTxGTKKaye-kCA4Ou-sh6rpg_6K0JAfiZhqAyvw,37307
 torchrec/distributed/collective_utils.py,sha256=r7Aawq-KSVC-HjjEd6U8k0vNnRMx_-8_sAhYdElGaJw,2069
 torchrec/distributed/comm.py,sha256=21Std9n_HJCF3Nsw9O4yQQOJuL2DUVzZzoRhH3M6my8,4988
-torchrec/distributed/comm_ops.py,sha256=C63THNJOnJWkfRYGuNw5opFyKsdlpZs9_23u-DHSrAQ,55918
-torchrec/distributed/dist_data.py,sha256=mpmO_oiZKCebKbYkXHu9CGwSHWLo-5w4cDj-tYvQ6kU,36967
+torchrec/distributed/comm_ops.py,sha256=1GOJQEoRPystHePHLDSf-qavv84MkVwSqLVgYSdg6LU,56136
+torchrec/distributed/dist_data.py,sha256=qGIhMurMuSK45hfGCZBvTUIdOJ4YPrnNlzYrHrQ_oIo,37051
 torchrec/distributed/embedding.py,sha256=ctrHHtWFq5ITPOucAbO2UQ2cRdeDPKRFyHd04lhYmPo,32295
 torchrec/distributed/embedding_kernel.py,sha256=9OA5PDofQZ-ZRORNR4Ca1pJoEZk5Oj2gE6axYwcXIds,4947
 torchrec/distributed/embedding_lookup.py,sha256=-rExTGufuH2qcRzeR2FSSdB9TfqsnVXdz-lghXceVZs,29185
-torchrec/distributed/embedding_sharding.py,sha256=X088TGuzLb3ou2LQWokn3MNYvo3bNpZT5RzdT1bz9JI,19035
+torchrec/distributed/embedding_sharding.py,sha256=Uvybimnwvf991uX840txCxO0xfKThKXwW-bqh9__3EY,20950
 torchrec/distributed/embedding_tower_sharding.py,sha256=eDHPHzjCstJJRJlE-n8a9lC54lCDsi1iAt4SmkLfza0,36853
 torchrec/distributed/embedding_types.py,sha256=QbDVqeT2wb1RpnGZxrFtFdHYDsOHKW7SH8fLWmN_d0E,15030
 torchrec/distributed/embeddingbag.py,sha256=3r_m4bnmyNWPI82_QDZ6vuZVuXnn8siPTDdMmhpX68o,37369
 torchrec/distributed/fbgemm_qcomm_codec.py,sha256=StYltKC6Eq6SE_YiX6GsVW3ZF0VyqTcGHXuCYmPAFlU,7373
 torchrec/distributed/fp_embeddingbag.py,sha256=sC7ZIECzJtwn5LNRrUzf0OH7phI3kQHqQ6wBPaGGvPQ,6137
 torchrec/distributed/fused_embedding.py,sha256=1VJeW5Dl7EFMvyOfhBvDKZlp39GYucBo8vNFJY2alFI,5243
 torchrec/distributed/fused_embeddingbag.py,sha256=tG_BrUlCdsek87jgHPKbxg3-z13sAgSWPNRBArf2_ss,5080
@@ -35,15 +35,15 @@
 torchrec/distributed/model_parallel.py,sha256=VV9Vsyas0VKNHr8sX9pq1iT_xJ2b9xjenY3pYAY5HYw,19750
 torchrec/distributed/quant_embedding.py,sha256=9hTS4oUYkoCEHInYO7r0IQjfAJi4fsw5WNiTOHGUJuM,20748
 torchrec/distributed/quant_embedding_kernel.py,sha256=VsROr4bXBkYysS0H7NnlZzF7IvGgZFtgws5jsnPf6g4,15112
 torchrec/distributed/quant_embeddingbag.py,sha256=SJPY-nL95LKozfT8iR4SHt1BNLx4OcoAfFvqKzy1Gf4,12621
 torchrec/distributed/quant_state.py,sha256=PDQ7qUwhFt-q1WvTkoSbWE3rotiFOYyKNFsQ5vJQ36U,13928
 torchrec/distributed/shard.py,sha256=4Dr5ixWCoMEFEuL5WN4fL2gIdl9wmSUjZWsiF-kdCdQ,9261
 torchrec/distributed/sharding_plan.py,sha256=DftThfyzPvfFzg7SvqsNo7X5n-1rxWhp1RlcQCcpq2o,19646
-torchrec/distributed/train_pipeline.py,sha256=VrJotTK7rSiVf0EWVlw3weE7jtkahgbtLju28y2PnuY,32061
+torchrec/distributed/train_pipeline.py,sha256=JTFXhso9xuGusvWcSYYTPptcnmn8TEPk0Xkq83AlF8k,39755
 torchrec/distributed/types.py,sha256=aqJ4ipEsMTy9WoTWCRfXg0KhHNcXYC-QRdBp0Db2rHY,27865
 torchrec/distributed/utils.py,sha256=dwNuXu5_OOK0YsGZhGPIdtZu4GzvT6X1MPsARINy10U,15470
 torchrec/distributed/composable/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/composable/table_batched_embedding_slice.py,sha256=x439M8TTXQtzoihan1OKKbmGYkqJlAxxTHCDz5295RY,3207
 torchrec/distributed/planner/__init__.py,sha256=UWnxb-SuE211uJGdwtSkKRVADT3plQozB2l6fvs6Ve0,1025
 torchrec/distributed/planner/constants.py,sha256=MkeVqYO2QGg57i6fs29lZb2dScaaR9mdQVsee4NxyFc,3135
 torchrec/distributed/planner/enumerators.py,sha256=JoQ1WcrU12Fyy57aBHitcKIpBTASJVnyLhkOxN7V4J4,11430
@@ -137,12 +137,12 @@
 torchrec/quant/__init__.py,sha256=A6NIA6ztq6iP1JTLRLNzlgnCcd-LaN8efnxGub3Ii4A,1140
 torchrec/quant/embedding_modules.py,sha256=-_5_HaoNpt3AmL7fapO2Wj2r6WPjbq2VvATNmGhrai0,26618
 torchrec/quant/utils.py,sha256=rcyo5LDcLK49VLs6ZFxOHeutblWZunDAM_T-0NsraDE,4292
 torchrec/sparse/__init__.py,sha256=dLqSye4Jo6obnNNTUKdPDxPQb9sL2U4weemSn-DjpYk,1163
 torchrec/sparse/jagged_tensor.py,sha256=__iBD2tBQE9ZZpxcOn8RVmDF9gognnyUlmMjedHxJ4E,56412
 torchrec/sparse/test_utils/__init__.py,sha256=BLxfGKJvwjjCiQM64O5wGAA_Cea0sG-buw9lTDWuqug,1430
 torchrec/test_utils/__init__.py,sha256=JncJcXS4N3gI7-fsizQ2-qiWM6MhIrpvskF_9gDf0Go,5661
-torchrec_nightly-2023.7.19.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
-torchrec_nightly-2023.7.19.dist-info/METADATA,sha256=Yxna7oVU-jV0UzmVgQl7ESQJPRDhzEMjBEEffcwLZGo,5012
-torchrec_nightly-2023.7.19.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
-torchrec_nightly-2023.7.19.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
-torchrec_nightly-2023.7.19.dist-info/RECORD,,
+torchrec_nightly-2023.7.21.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
+torchrec_nightly-2023.7.21.dist-info/METADATA,sha256=i7DTRHZ2sKT6TgvENgDU5vUMl11jfVROiikoXQji2C4,5012
+torchrec_nightly-2023.7.21.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+torchrec_nightly-2023.7.21.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
+torchrec_nightly-2023.7.21.dist-info/RECORD,,
```

